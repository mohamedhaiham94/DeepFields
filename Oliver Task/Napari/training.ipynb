{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset trovato: 4 coppie (img, mask)\n",
      "Epoch [1/30] Loss: 0.514876\n",
      "Epoch [2/30] Loss: 0.205419\n",
      "Epoch [3/30] Loss: 0.088131\n",
      "Epoch [4/30] Loss: 0.197348\n",
      "Epoch [5/30] Loss: 0.209756\n",
      "Epoch [6/30] Loss: 0.055685\n",
      "Epoch [7/30] Loss: 0.047094\n",
      "Epoch [8/30] Loss: 0.133145\n",
      "Epoch [9/30] Loss: 0.092881\n",
      "Epoch [10/30] Loss: 0.083119\n",
      "Epoch [11/30] Loss: 0.059478\n",
      "Epoch [12/30] Loss: 0.057341\n",
      "Epoch [13/30] Loss: 0.034911\n",
      "Epoch [14/30] Loss: 0.018312\n",
      "Epoch [15/30] Loss: 0.011621\n",
      "Epoch [16/30] Loss: 0.012398\n",
      "Epoch [17/30] Loss: 0.012488\n",
      "Epoch [18/30] Loss: 0.012598\n",
      "Epoch [19/30] Loss: 0.046125\n",
      "Epoch [20/30] Loss: 0.035092\n",
      "Epoch [21/30] Loss: 0.022733\n",
      "Epoch [22/30] Loss: 0.014331\n",
      "Epoch [23/30] Loss: 0.011216\n",
      "Epoch [24/30] Loss: 0.011062\n",
      "Epoch [25/30] Loss: 0.012562\n",
      "Epoch [26/30] Loss: 0.009163\n",
      "Epoch [27/30] Loss: 0.005524\n",
      "Epoch [28/30] Loss: 0.005950\n",
      "Epoch [29/30] Loss: 0.002939\n",
      "Epoch [30/30] Loss: 0.007011\n",
      "Model saved in model_iter_inout.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ==============================================\n",
    "# 1) Config\n",
    "# ==============================================\n",
    "IMAGES_DIR = \"min_results\"      # Folder with original images \n",
    "LABELS_DIR = \"min_mask_1\"       # Folder with masks (1=in focus, 3=out-of-focus, 0=unlabeled)\n",
    "MODEL_SAVE_PATH = \"model_iter_inout.pt\"\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# 2) Dataset\n",
    "# ==============================================\n",
    "class TwoClassDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Read tuple (image, mask).\n",
    "    Mask can have the following values:\n",
    "      - 0 => unlabeled (then ignore)\n",
    "      - 1 => in focus => class 0\n",
    "      - 3 => out of focus => class 1\n",
    "    \"\"\"\n",
    "    def __init__(self, images_dir, labels_dir):\n",
    "        super().__init__()\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "\n",
    "        all_img_paths = sorted(glob.glob(os.path.join(images_dir, \"*.tiff\")))\n",
    "        for img_path in all_img_paths:\n",
    "            base = os.path.basename(img_path)\n",
    "            label_path = os.path.join(labels_dir, base)\n",
    "            if os.path.exists(label_path):\n",
    "                self.imgs.append(img_path)\n",
    "                self.labels.append(label_path)\n",
    "\n",
    "        print(f\"Dataset trovato: {len(self.imgs)} coppie (img, mask)\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "        lbl_path = self.labels[idx]\n",
    "\n",
    "        img = tifffile.imread(img_path).astype(np.float32)\n",
    "        lbl = tifffile.imread(lbl_path).astype(np.float32)\n",
    "\n",
    "        # Just a sanity chekc\n",
    "        if img.ndim == 3:\n",
    "            img = img[..., 0]\n",
    "        if lbl.ndim == 3:\n",
    "            lbl = lbl[..., 0]\n",
    "\n",
    "        # Normalizing the image if max>1 \n",
    "        if img.max() > 1:\n",
    "            img /= 255.0\n",
    "\n",
    "        # lbl avrÃ  [0,1,3]. Tenuta in float\n",
    "        # shape -> (1,H,W)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "\n",
    "        # Tensor conversion\n",
    "        img_t = torch.from_numpy(img)\n",
    "\n",
    "        lbl_t = torch.from_numpy(lbl)  # shape (H,W) in float\n",
    "        return img_t, lbl_t\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# 3) Model with 2 classes (2 channels out)\n",
    "# ==============================================\n",
    "class Simple2ClassNet(nn.Module):\n",
    "    \"\"\"\n",
    "    2 layers conv, output=2 channels (logits)\n",
    "    channel 0 => \"in focus\"\n",
    "    channel 1 => \"out of focus\"\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 2, 3, padding=1)  # 2 channels\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x  # shape [B,2,H,W]\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# 4) Training with partial cross-entropy\n",
    "# ==============================================\n",
    "def train_inout(epochs=10, lr=1e-3, batch_size=2):\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    ds = TwoClassDataset(IMAGES_DIR, LABELS_DIR)\n",
    "    if len(ds) == 0:\n",
    "        print(\"No data!\")\n",
    "        return\n",
    "\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    model = Simple2ClassNet().to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    def partial_cross_entropy(logits, lbls):\n",
    "        \"\"\"\n",
    "        logits: shape [B,2,H,W]\n",
    "        lbls: shape [B,H,W] con valori 0,1,3\n",
    "          0 => unlabeled\n",
    "          1 => classe=0\n",
    "          3 => classe=1\n",
    "        \"\"\"\n",
    "        b, c, h, w = logits.shape\n",
    "        lbl_resh = lbls.view(b, h, w)  # (B,H,W)\n",
    "        logits_resh = logits.view(b, c, h, w)\n",
    "\n",
    "        # building mask unlabeled\n",
    "        mask_unl = (lbl_resh == 0.0)\n",
    "        # Building a class target \n",
    "        cl_target = torch.where(lbl_resh == 1.0, 0, 1)  # default out-of-focus=1\n",
    "\n",
    "        # flatten\n",
    "        logits_resh = logits_resh.permute(0,2,3,1).contiguous()  # (B,H,W,2)\n",
    "        logits_resh = logits_resh.view(-1, 2)                    # (B*H*W, 2)\n",
    "        cl_target = cl_target.view(-1)                           # (B*H*W)\n",
    "\n",
    "        mask_unl = mask_unl.view(-1)  # bool\n",
    "\n",
    "        # if all pixel are unlabeled, return 0 \n",
    "        labeled_idx = (~mask_unl).nonzero(as_tuple=True)[0]\n",
    "        if labeled_idx.numel() == 0:\n",
    "            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "\n",
    "        # Filtering logits and target \n",
    "        logits_lab = logits_resh[labeled_idx]    # (num_lab, 2)\n",
    "        target_lab = cl_target[labeled_idx]      # (num_lab)\n",
    "\n",
    "        # cross_entropy standard\n",
    "        ce = F.cross_entropy(logits_lab, target_lab)\n",
    "        return ce\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for imgs, lbls in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            lbls = lbls.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(imgs)\n",
    "            loss = partial_cross_entropy(logits, lbls)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "    # Save\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"Model saved in {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# =================================================\n",
    "if __name__ == \"__main__\":\n",
    "    train_inout(epochs=30, lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvproj310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
