{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03dd9b85",
   "metadata": {},
   "source": [
    "### Load Images and run Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8413f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import napari\n",
    "import numpy as np\n",
    "from skimage.io import imread_collection\n",
    "import glob\n",
    "\n",
    "# Usa glob per ottenere correttamente i percorsi di tutte le immagini RGB\n",
    "rgb_image_paths = sorted(glob.glob(\"../data/RGBintegrals/layer_*.png\"))\n",
    "\n",
    "# Carica le immagini RGB come stack 3D\n",
    "rgb_images = imread_collection(rgb_image_paths).concatenate()\n",
    "\n",
    "# Crea il viewer Napari con le immagini RGB\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(rgb_images, name=\"RGB Integrals\", rgb=True)\n",
    "\n",
    "# Crea layer vuoto per annotazioni manuali\n",
    "labels_layer = viewer.add_labels(np.zeros(rgb_images.shape[:-1], dtype=int), name=\"Manual Labels\")\n",
    "\n",
    "napari.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adadbc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Layer 0] modello caricato, proseguo fine-tuning.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 153\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# train se ci sono annotazioni, altrimenti prendi modello “più vicino”\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.any(mask > \u001b[32m0\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    155\u001b[39m     available = [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers) \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(os.path.join(MODEL_DIR, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlayer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pt\u001b[39m\u001b[33m\"\u001b[39m))]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(layer_idx, img, mask, model_path, epochs, lr)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m imgs, lbls \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m         imgs, lbls = \u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, lbls.to(device)\n\u001b[32m    114\u001b[39m         optim.zero_grad()\n\u001b[32m    115\u001b[39m         loss = partial_cross_entropy(model(imgs), lbls)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.io import imread\n",
    "import napari                         # <-- NEW: per aggiornare/creare il layer di predizione\n",
    "\n",
    "IMAGES_DIR = \"../data/data1/min_results\"\n",
    "MODEL_DIR  = \"models_iterative\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "#  MODELLINO 2-CLASSI\n",
    "# ---------------------------------------------------------------------\n",
    "class Simple2ClassNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 2, 3, padding=1)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "#  LOSS: usa solo i pixel annotati (lbl == 2 o 3)\n",
    "# ---------------------------------------------------------------------\n",
    "def partial_cross_entropy(logits, lbls, min_w=1.0, max_w=200.0):\n",
    "    unlabeled = (lbls == 0)\n",
    "    # print(f\"  Loss - Valori unici etichette input (lbls): {torch.unique(lbls)}\") \n",
    "    target    = torch.where(lbls == 2, 1, 0)          # 2→1 (fuoco), 3→0 (sfocato)\n",
    "    # print(f\"  Loss - Valori unici target (0/1): {torch.unique(target)}\")\n",
    "\n",
    "    # raddrizza tensori\n",
    "    logits = logits.permute(0, 2, 3, 1).reshape(-1, 2)\n",
    "    target = target.reshape(-1)\n",
    "    mask   = (~unlabeled.reshape(-1))\n",
    "\n",
    "    # print(f\"  Loss - Numero pixel etichettati (mask.sum()): {mask.sum().item()}\")\n",
    "\n",
    "    if not mask.any():\n",
    "        return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "\n",
    "    # pixel etichettati\n",
    "    logits_lab = logits[mask]\n",
    "    target_lab = target[mask]\n",
    "    # print(f\"  Loss - Shape target_lab: {target_lab.shape}\")\n",
    "\n",
    "    # ---- P E S I  D Y N A M I C I -----------------------------------\n",
    "    n_total = target_lab.numel()\n",
    "    n_pos   = (target_lab == 1).sum()\n",
    "    n_neg   = n_total - n_pos\n",
    "\n",
    "    # print(f\"  Loss - n_total: {n_total}, n_pos (in-focus): {n_pos.item()}, n_neg (out-of-focus): {n_neg.item()}\") # <-- CONTROLLA QUI!\n",
    "\n",
    "    # evita divisione per zero\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        weight = torch.tensor([1.0, 1.0], device=logits.device)\n",
    "    else:\n",
    "        # peso inversamente proporzionale alla frequenza\n",
    "        w_neg = torch.clamp(n_total / (2.0 * n_neg.float()), min=min_w, max=max_w)\n",
    "        w_pos = torch.clamp(n_total / (2.0 * n_pos.float()), min=min_w, max=max_w)\n",
    "        weight = torch.tensor([w_neg, w_pos], device=logits.device)\n",
    "        # print(f\"  Loss - Pesi calcolati: {weight}\")\n",
    "\n",
    "    loss = F.cross_entropy(logits_lab, target_lab, weight=weight, reduction='mean')\n",
    "    # print(f\"  Loss - Valore loss calcolato: {loss.item()}\") # <-- CONTROLLA QUI!\n",
    "    return loss\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "#  DATASET “one-shot” (un’unica immagine + mask)\n",
    "# ---------------------------------------------------------------------\n",
    "class NapariDataset(Dataset):\n",
    "    def __init__(self, image, mask):\n",
    "        self.image = image.astype(np.float32) / (255.0 if image.max() > 1 else 1.0)\n",
    "        self.mask  = mask.astype(np.float32)\n",
    "\n",
    "    def __len__(self):  return 1\n",
    "    def __getitem__(self, idx):\n",
    "        img = np.expand_dims(self.image, axis=0)       # (1, H, W)\n",
    "        return torch.from_numpy(img), torch.from_numpy(self.mask)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "#  TRAIN O FINE-TUNE DEL MODELLO PER UN LAYER\n",
    "# ---------------------------------------------------------------------\n",
    "def train_model(layer_idx, img, mask, model_path, epochs=100, lr=1e-5):\n",
    "    model = Simple2ClassNet().to(device)\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        print(f\"[Layer {layer_idx}] modello caricato, proseguo fine-tuning.\")\n",
    "    else:\n",
    "        print(f\"[Layer {layer_idx}] nuovo modello creato.\")\n",
    "\n",
    "    loader = DataLoader(NapariDataset(img, mask), batch_size=1, shuffle=False)\n",
    "    optim  = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        for imgs, lbls in loader:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            optim.zero_grad()\n",
    "            loss = partial_cross_entropy(model(imgs), lbls)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        # print(f\"[Layer {layer_idx}] epoch {ep+1}/{epochs} – loss {loss.item():.5f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "#  INFERENZA SINGOLO LAYER\n",
    "# ---------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def infer_mask(model, img):\n",
    "    t = torch.from_numpy(img).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "    pred = torch.softmax(model(t), dim=1).argmax(1).squeeze(0).cpu().numpy()\n",
    "    # print(f\"  Inferenza - Valori unici nella predizione: {np.unique(pred)}\") # <-- CONTROLLA QUI!\n",
    "    return pred   # 0 = out-of-focus, 1 = in-focus\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "#  ---- MAIN LOOP ----\n",
    "# ---------------------------------------------------------------------\n",
    "labels_layer        = napari.current_viewer().layers['Manual Labels']\n",
    "labels_data         = labels_layer.data                  # (Z, H, W) con 0/2/3\n",
    "variance_paths      = sorted(glob.glob(f\"{IMAGES_DIR}/variance_image_*_min_grayscale.tiff\"))\n",
    "num_layers, H, W    = labels_data.shape\n",
    "final_prediction    = np.zeros_like(labels_data)         # (Z, H, W) 0/1\n",
    "\n",
    "for z in range(num_layers):\n",
    "    mask   = labels_data[z]\n",
    "    # print(f\"[Layer {z}] Valori unici nella maschera manuale: {np.unique(mask)}\")\n",
    "    img    = imread(variance_paths[max(z, 0)]).astype(np.float32) / 255.0\n",
    "    # print(f\"[Layer {z}] Immagine caricata - min: {img.min()}, max: {img.max()}, mean: {img.mean()}\") \n",
    "    mpath  = os.path.join(MODEL_DIR, f\"layer_{z}.pt\")\n",
    "\n",
    "    # train se ci sono annotazioni, altrimenti prendi modello “più vicino”\n",
    "    if np.any(mask > 0):\n",
    "        model = train_model(z, img, mask, mpath)\n",
    "    else:\n",
    "        available = [i for i in range(num_layers) if os.path.exists(os.path.join(MODEL_DIR, f\"layer_{i}.pt\"))]\n",
    "        if not available:\n",
    "            # print(f\"[Layer {z}] nessun modello disponibile - skippato.\")\n",
    "            continue\n",
    "        nearest = min(available, key=lambda i: abs(z - i))\n",
    "        mpath   = os.path.join(MODEL_DIR, f\"layer_{nearest}.pt\")\n",
    "        model   = Simple2ClassNet().to(device)\n",
    "        model.load_state_dict(torch.load(mpath, map_location=device))\n",
    "        # print(f\"[Layer {z}] uso modello del layer {nearest}.\")\n",
    "\n",
    "    final_prediction[z] = infer_mask(model, img)         # 0/1\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "#  -----  AGGIORNA / CREA LAYER “Model Pred” IN NAPARI  -----\n",
    "# ---------------------------------------------------------------------\n",
    "viewer  = napari.current_viewer()\n",
    "PRED_L  = 'Model Pred'\n",
    "\n",
    "if PRED_L in viewer.layers:\n",
    "    viewer.layers[PRED_L].data = final_prediction          # aggiorna dati\n",
    "else:\n",
    "    # 1) crea il layer (senza 'color='!)\n",
    "    pred_layer = viewer.add_labels(\n",
    "        final_prediction,\n",
    "        name    = PRED_L,\n",
    "        opacity = 0.40,        # trasparenza\n",
    "    )\n",
    "\n",
    "    # 2) imposta la tavolozza colori a posteriori\n",
    "    #    (0 = sfocato → magenta, 1 = a fuoco → lime)\n",
    "    pred_layer.color = {0: 'magenta', 1: 'lime'}\n",
    "\n",
    "    # 3) blocca l’editing per evitare tocchi accidentali\n",
    "    pred_layer.editable = False\n",
    "\n",
    "print(\"✓ Predizione aggiornata sul layer 'Model Pred'. \"\n",
    "      \"Accendi/spegni la trasparenza per controllare gli errori e \"\n",
    "      \"correggi soltanto sul layer 'Manual Labels'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb726a9e",
   "metadata": {},
   "source": [
    "## Inference different from zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37573049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    # Ottieni il viewer Napari corrente\n",
    "    viewer = napari.current_viewer()\n",
    "\n",
    "    if viewer is None:\n",
    "        print(\"Errore: Nessun viewer Napari attivo.\")\n",
    "    else:\n",
    "        layer_name = 'Model Pred'\n",
    "        if layer_name in viewer.layers:\n",
    "            # Accedi al layer delle predizioni\n",
    "            prediction_layer = viewer.layers[layer_name]\n",
    "\n",
    "            # Ottieni i dati (l'array NumPy con shape (Z, H, W))\n",
    "            prediction_data = prediction_layer.data\n",
    "\n",
    "            # --- Modifica per contare i layer con valori > 0 ---\n",
    "            num_total_layers = prediction_data.shape[0]\n",
    "            layers_with_non_zeros_count = 0\n",
    "            indices_with_non_zeros = [] # Lista per tenere traccia degli indici (opzionale)\n",
    "\n",
    "            # Itera su ogni singolo layer (slice 2D)\n",
    "            for i in range(num_total_layers):\n",
    "                # Prendi i dati del layer corrente\n",
    "                current_layer_data = prediction_data[i]\n",
    "                # Controlla se ESISTE ALMENO UN valore > 0 in questo layer\n",
    "                if np.any(current_layer_data > 0):\n",
    "                    layers_with_non_zeros_count += 1\n",
    "                    indices_with_non_zeros.append(i) # Aggiunge l'indice del layer alla lista\n",
    "\n",
    "            # --- Fine Modifica ---\n",
    "\n",
    "            # Stampa il riepilogo\n",
    "            print(f\"Controllo '{layer_name}':\")\n",
    "            print(f\"- Numero totale di layer: {num_total_layers}\")\n",
    "\n",
    "            if layers_with_non_zeros_count > 0:\n",
    "                print(f\"- Numero di layer contenenti valori diversi da 0: {layers_with_non_zeros_count}\")\n",
    "                # Puoi anche stampare gli indici se ti interessa sapere *quali* layer sono\n",
    "                print(f\"- Indici dei layer con valori > 0: {indices_with_non_zeros}\")\n",
    "\n",
    "                # Conferma i valori unici globali trovati\n",
    "                unique_values = np.unique(prediction_data)\n",
    "                print(f\"- Valori unici trovati nell'intero stack: {unique_values}\")\n",
    "            else:\n",
    "                # Questo caso non dovrebbe verificarsi dato il tuo output precedente, ma lo teniamo per completezza\n",
    "                print(\"- Tutti i layer contengono solo il valore 0.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Errore: Il layer '{layer_name}' non esiste nel viewer.\")\n",
    "            print(\"Assicurati di aver eseguito lo script di training/predizione.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Si è verificato un errore durante l'accesso a Napari o al layer: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a62ab",
   "metadata": {},
   "source": [
    "## Train a Unique Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbdbf239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "--- Step 1: Collecting Annotated Data ---\n",
      "Scanning layers for annotations...\n",
      "\n",
      "Raccolti 7 layer annotati per l'addestramento: [0, 10, 15, 17, 33, 36, 45]\n",
      "\n",
      "--- Step 2: Training Single Model ---\n",
      "Caricato modello esistente da: models_iterative/unique_focus_model.pt. Si procede al fine-tuning.\n",
      "Inizio addestramento su 7 campioni (layer annotati) per 60 epoche...\n",
      "Epoch 1/60 - Avg Loss: 0.543709\n",
      "Epoch 2/60 - Avg Loss: 0.445135\n",
      "Epoch 3/60 - Avg Loss: 0.537893\n",
      "Epoch 4/60 - Avg Loss: 0.536940\n",
      "Epoch 5/60 - Avg Loss: 0.486156\n",
      "Epoch 6/60 - Avg Loss: 0.467444\n",
      "Epoch 7/60 - Avg Loss: 0.536655\n",
      "Epoch 8/60 - Avg Loss: 0.494550\n",
      "Epoch 9/60 - Avg Loss: 0.528136\n",
      "Epoch 10/60 - Avg Loss: 0.505929\n",
      "Epoch 11/60 - Avg Loss: 0.537301\n",
      "Epoch 12/60 - Avg Loss: 0.534794\n",
      "Epoch 13/60 - Avg Loss: 0.505544\n",
      "Epoch 14/60 - Avg Loss: 0.473211\n",
      "Epoch 15/60 - Avg Loss: 0.526929\n",
      "Epoch 16/60 - Avg Loss: 0.504592\n",
      "Epoch 17/60 - Avg Loss: 0.518113\n",
      "Epoch 18/60 - Avg Loss: 0.546298\n",
      "Epoch 19/60 - Avg Loss: 0.505221\n",
      "Epoch 20/60 - Avg Loss: 0.536083\n",
      "Epoch 21/60 - Avg Loss: 0.534858\n",
      "Epoch 22/60 - Avg Loss: 0.530678\n",
      "Epoch 23/60 - Avg Loss: 0.503865\n",
      "Epoch 24/60 - Avg Loss: 0.530644\n",
      "Epoch 25/60 - Avg Loss: 0.494361\n",
      "Epoch 26/60 - Avg Loss: 0.524259\n",
      "Epoch 27/60 - Avg Loss: 0.465875\n",
      "Epoch 28/60 - Avg Loss: 0.530945\n",
      "Epoch 29/60 - Avg Loss: 0.495218\n",
      "Epoch 30/60 - Avg Loss: 0.537345\n",
      "Epoch 31/60 - Avg Loss: 0.534684\n",
      "Epoch 32/60 - Avg Loss: 0.514845\n",
      "Epoch 33/60 - Avg Loss: 0.533900\n",
      "Epoch 34/60 - Avg Loss: 0.504232\n",
      "Epoch 35/60 - Avg Loss: 0.513808\n",
      "Epoch 36/60 - Avg Loss: 0.531914\n",
      "Epoch 37/60 - Avg Loss: 0.527110\n",
      "Epoch 38/60 - Avg Loss: 0.501836\n",
      "Epoch 39/60 - Avg Loss: 0.469899\n",
      "Epoch 40/60 - Avg Loss: 0.532693\n",
      "Epoch 41/60 - Avg Loss: 0.541656\n",
      "Epoch 42/60 - Avg Loss: 0.518572\n",
      "Epoch 43/60 - Avg Loss: 0.514057\n",
      "Epoch 44/60 - Avg Loss: 0.520485\n",
      "Epoch 45/60 - Avg Loss: 0.525409\n",
      "Epoch 46/60 - Avg Loss: 0.511565\n",
      "Epoch 47/60 - Avg Loss: 0.536093\n",
      "Epoch 48/60 - Avg Loss: 0.521266\n",
      "Epoch 49/60 - Avg Loss: 0.524690\n",
      "Epoch 50/60 - Avg Loss: 0.478517\n",
      "Epoch 51/60 - Avg Loss: 0.510832\n",
      "Epoch 52/60 - Avg Loss: 0.503029\n",
      "Epoch 53/60 - Avg Loss: 0.522980\n",
      "Epoch 54/60 - Avg Loss: 0.454532\n",
      "Epoch 55/60 - Avg Loss: 0.527467\n",
      "Epoch 56/60 - Avg Loss: 0.536110\n",
      "Epoch 57/60 - Avg Loss: 0.474227\n",
      "Epoch 58/60 - Avg Loss: 0.501228\n",
      "Epoch 59/60 - Avg Loss: 0.492968\n",
      "Epoch 60/60 - Avg Loss: 0.513905\n",
      "Addestramento completato in 45.94 secondi.\n",
      "Modello unico salvato in: models_iterative/unique_focus_model.pt\n",
      "\n",
      "--- Step 3: Performing Inference ---\n",
      "Applicazione del modello unico a tutti i 100 layer...\n",
      "Inferenza completata in 0.71 secondi.\n",
      "\n",
      "--- Step 4: Updating Napari Layer 'Model Pred Unique' ---\n",
      "Aggiornamento layer esistente 'Model Pred Unique'...\n",
      "Layer aggiornato.\n",
      "\n",
      "✓ Predizione 'unica' aggiornata sul layer 'Model Pred Unique'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import tifffile\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.io import imread\n",
    "import napari\n",
    "import time # Per misurare il tempo\n",
    "\n",
    "# --- Assicurati che queste siano definite prima ---\n",
    "# class Simple2ClassNet(nn.Module): ...\n",
    "# def partial_cross_entropy(logits, lbls, min_w=1.0, max_w=50.0): ... # Nota: max_w aumentato!\n",
    "# @torch.no_grad() def infer_mask(model, img): ...\n",
    "# -------------------------------------------------\n",
    "\n",
    "# --- Configurazioni ---\n",
    "IMAGES_DIR = \"../data/data1/min_results\"\n",
    "MODEL_DIR  = \"models_iterative\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Nuovo nome per il layer di output\n",
    "UNIQUE_PRED_LAYER_NAME = 'Model Pred Unique'\n",
    "\n",
    "# --- Dataset Modificato per gestire Multipli Layer ---\n",
    "class NapariMultiLayerDataset(Dataset):\n",
    "    def __init__(self, images_list, masks_list):\n",
    "        # Normalizza tutte le immagini in input\n",
    "        self.images = []\n",
    "        for img in images_list:\n",
    "             # Assicurati che sia float32 e normalizzato\n",
    "             img_float = img.astype(np.float32)\n",
    "             # Normalizza dividendo per 255 se i valori sono alti, altrimenti per 1\n",
    "             norm_factor = 255.0 if img.max() > 1.5 else 1.0 # Leggermente più robusto di \"> 1\"\n",
    "             self.images.append(img_float / norm_factor)\n",
    "\n",
    "        # Le maschere rimangono come sono (int o float, la loss le gestirà)\n",
    "        self.masks = [m.astype(np.int64) for m in masks_list] # Usa int64 per compatibilità loss PyTorch\n",
    "\n",
    "    def __len__(self):\n",
    "        # La lunghezza è il numero di coppie immagine/maschera fornite\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Prende l'immagine e la maschera all'indice specificato\n",
    "        img = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "\n",
    "        # Aggiunge la dimensione del canale all'immagine (richiesta da Conv2d)\n",
    "        img_tensor = torch.from_numpy(np.expand_dims(img, axis=0)) # Shape: (1, H, W)\n",
    "        mask_tensor = torch.from_numpy(mask) # Shape: (H, W)\n",
    "\n",
    "        return img_tensor, mask_tensor\n",
    "\n",
    "# --- 1. Raccogli TUTTI i dati annotati ---\n",
    "print(\"--- Step 1: Collecting Annotated Data ---\")\n",
    "try:\n",
    "    viewer = napari.current_viewer()\n",
    "    if viewer is None:\n",
    "        raise RuntimeError(\"Nessun viewer Napari attivo.\")\n",
    "\n",
    "    labels_layer = viewer.layers['Manual Labels']\n",
    "    labels_data = labels_layer.data # Shape (Z, H, W)\n",
    "    variance_paths = sorted(glob.glob(f\"{IMAGES_DIR}/variance_image_*_min_grayscale.tiff\"))\n",
    "    num_layers, H, W = labels_data.shape\n",
    "\n",
    "    if len(variance_paths) != num_layers:\n",
    "         print(f\"Warning: Numero di immagini ({len(variance_paths)}) non corrisponde al numero di layer nelle etichette ({num_layers}). Potrebbero esserci errori.\")\n",
    "\n",
    "    all_images_to_train = []\n",
    "    all_masks_to_train = []\n",
    "    annotated_layer_indices = []\n",
    "\n",
    "    print(\"Scanning layers for annotations...\")\n",
    "    for z in range(num_layers):\n",
    "        mask = labels_data[z]\n",
    "        # Controlla se ci sono etichette DIVERSE da 0\n",
    "        if np.any(mask != 0):\n",
    "            # Assicurati che ci siano etichette 2 o 3 (o entrambe)\n",
    "            unique_labels_in_mask = np.unique(mask)\n",
    "            if 2 in unique_labels_in_mask or 3 in unique_labels_in_mask:\n",
    "                try:\n",
    "                    # Carica l'immagine corrispondente\n",
    "                    img = imread(variance_paths[z])\n",
    "                    all_images_to_train.append(img) # Normalizzazione fatta nel Dataset\n",
    "                    all_masks_to_train.append(mask)\n",
    "                    annotated_layer_indices.append(z)\n",
    "                    # print(f\"  Layer {z}: Found annotations {unique_labels_in_mask}. Added to training set.\")\n",
    "                except IndexError:\n",
    "                    print(f\"Warning: Impossibile trovare variance_paths[{z}]. Salto layer {z} annotato.\")\n",
    "                except FileNotFoundError:\n",
    "                     print(f\"Warning: Immagine varianza non trovata per layer {z} annotato: {variance_paths[z]}. Salto.\")\n",
    "            # else:\n",
    "            #    print(f\"  Layer {z}: Contiene etichette diverse da 0, ma non 2 o 3 ({unique_labels_in_mask}). Escluso.\")\n",
    "        # else:\n",
    "        #    print(f\"  Layer {z}: Nessuna annotazione manuale trovata.\")\n",
    "\n",
    "\n",
    "    if not all_images_to_train:\n",
    "        print(\"\\nErrore: Nessun layer con annotazioni valide (2 o 3) trovato. Impossibile addestrare.\")\n",
    "        # Qui potresti voler uscire o gestire l'errore diversamente\n",
    "    else:\n",
    "        print(f\"\\nRaccolti {len(all_images_to_train)} layer annotati per l'addestramento: {annotated_layer_indices}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante la raccolta dati: {e}\")\n",
    "    # Esci o gestisci l'errore se non puoi continuare\n",
    "    all_images_to_train = [] # Assicura che non si proceda se c'è stato un errore\n",
    "\n",
    "# --- 2. Addestra UN SINGOLO Modello ---\n",
    "print(\"\\n--- Step 2: Training Single Model ---\")\n",
    "model = None # Inizializza a None\n",
    "unique_model_path = os.path.join(MODEL_DIR, \"unique_focus_model.pt\") # Nome file specifico\n",
    "\n",
    "if all_images_to_train: # Procedi solo se hai dati\n",
    "    epochs = 60 # Potrebbero servire più epoche per un dataset aggregato\n",
    "    lr = 1e-3\n",
    "    batch_size = 4 # Esempio; aggiusta in base alla memoria GPU/CPU\n",
    "    # Usa un valore alto per max_w come discusso!\n",
    "    loss_max_w = 50.0\n",
    "\n",
    "    # Istanzia il modello\n",
    "    model = Simple2ClassNet().to(device)\n",
    "\n",
    "    # Carica il modello se esiste già per fine-tuning\n",
    "    if os.path.exists(unique_model_path):\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(unique_model_path, map_location=device))\n",
    "            print(f\"Caricato modello esistente da: {unique_model_path}. Si procede al fine-tuning.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel caricamento del modello da {unique_model_path}. Si addestra da zero. Dettagli: {e}\")\n",
    "    else:\n",
    "        print(f\"Nessun modello pre-esistente trovato. Creazione nuovo modello unico.\")\n",
    "\n",
    "    # Crea Dataset e DataLoader\n",
    "    dataset = NapariMultiLayerDataset(all_images_to_train, all_masks_to_train)\n",
    "    # Shuffle=True è importante per l'addestramento\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0) # num_workers=0 per MPS\n",
    "\n",
    "    # Ottimizzatore\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    print(f\"Inizio addestramento su {len(dataset)} campioni (layer annotati) per {epochs} epoche...\")\n",
    "    start_time_train = time.time()\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train() # Imposta modalità training\n",
    "        epoch_loss_sum = 0.0\n",
    "        batches_processed = 0\n",
    "        for i, (imgs_batch, lbls_batch) in enumerate(loader):\n",
    "            # imgs_batch: (B, 1, H, W), lbls_batch: (B, H, W)\n",
    "            imgs_batch, lbls_batch = imgs_batch.to(device), lbls_batch.to(device)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            logits_batch = model(imgs_batch) # Output: (B, 2, H, W)\n",
    "\n",
    "            # Calcola la loss sul batch, usando i pesi aumentati\n",
    "            loss = partial_cross_entropy(logits_batch, lbls_batch, max_w=loss_max_w)\n",
    "\n",
    "            # Propaga l'errore solo se la loss è valida (es. se c'erano etichette nel batch)\n",
    "            if loss.requires_grad:\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                epoch_loss_sum += loss.item()\n",
    "                batches_processed += 1\n",
    "            # else:\n",
    "                # print(f\"  Batch {i}: Loss non richiede gradiente (probabilmente nessuna etichetta nel batch?).\")\n",
    "\n",
    "\n",
    "        avg_epoch_loss = epoch_loss_sum / batches_processed if batches_processed > 0 else 0\n",
    "        print(f\"Epoch {ep+1}/{epochs} - Avg Loss: {avg_epoch_loss:.6f}\")\n",
    "\n",
    "    end_time_train = time.time()\n",
    "    print(f\"Addestramento completato in {end_time_train - start_time_train:.2f} secondi.\")\n",
    "\n",
    "    # Salva il modello unico addestrato\n",
    "    try:\n",
    "        torch.save(model.state_dict(), unique_model_path)\n",
    "        print(f\"Modello unico salvato in: {unique_model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il salvataggio del modello: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Addestramento saltato perché non sono stati raccolti dati annotati validi.\")\n",
    "    # Prova a caricare il modello se esiste, altrimenti l'inferenza fallirà\n",
    "    if os.path.exists(unique_model_path):\n",
    "         model = Simple2ClassNet().to(device)\n",
    "         model.load_state_dict(torch.load(unique_model_path, map_location=device))\n",
    "         print(f\"Modello caricato da {unique_model_path} per inferenza (addestramento saltato).\")\n",
    "\n",
    "\n",
    "# --- 3. Esegui Inferenza su TUTTI i Layer con il Modello Unico ---\n",
    "print(\"\\n--- Step 3: Performing Inference ---\")\n",
    "# Assicurati che esista un modello (o addestrato ora o caricato)\n",
    "if model is None:\n",
    "     print(\"Errore: Nessun modello disponibile per l'inferenza.\")\n",
    "else:\n",
    "    model.eval() # Imposta modalità valutazione (importante!)\n",
    "    final_prediction = np.zeros_like(labels_data, dtype=np.int8) # Maschera 0/1\n",
    "\n",
    "    print(f\"Applicazione del modello unico a tutti i {num_layers} layer...\")\n",
    "    start_time_infer = time.time()\n",
    "\n",
    "    for z in range(num_layers):\n",
    "        try:\n",
    "            # Carica e normalizza l'immagine di varianza per il layer corrente\n",
    "            img = imread(variance_paths[z]).astype(np.float32)\n",
    "            img_norm = img / (255.0 if img.max() > 1.5 else 1.0)\n",
    "\n",
    "            # Esegui inferenza con il modello unico\n",
    "            prediction_mask = infer_mask(model, img_norm) # infer_mask ritorna array numpy 0/1\n",
    "            final_prediction[z] = prediction_mask.astype(np.int8)\n",
    "\n",
    "            # Stampa progresso ogni 10 layer (opzionale)\n",
    "            # if (z + 1) % 10 == 0 or z == num_layers - 1:\n",
    "            #     print(f\"  Inferenza completata per layer {z+1}/{num_layers}\")\n",
    "\n",
    "        except (FileNotFoundError, IndexError):\n",
    "            print(f\"Warning: Immagine varianza per layer {z} non trovata o indice errato. Inferenza saltata.\")\n",
    "            final_prediction[z] = 0 # Lascia il layer a 0 se l'immagine manca\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante l'inferenza per layer {z}: {e}. Inferenza saltata.\")\n",
    "            final_prediction[z] = 0\n",
    "\n",
    "    end_time_infer = time.time()\n",
    "    print(f\"Inferenza completata in {end_time_infer - start_time_infer:.2f} secondi.\")\n",
    "\n",
    "    # --- 4. Aggiorna/Crea Layer in Napari ---\n",
    "    print(f\"\\n--- Step 4: Updating Napari Layer '{UNIQUE_PRED_LAYER_NAME}' ---\")\n",
    "    if viewer: # Controlla se il viewer esiste ancora\n",
    "        if UNIQUE_PRED_LAYER_NAME in viewer.layers:\n",
    "            print(f\"Aggiornamento layer esistente '{UNIQUE_PRED_LAYER_NAME}'...\")\n",
    "            try:\n",
    "                viewer.layers[UNIQUE_PRED_LAYER_NAME].data = final_prediction\n",
    "                # Riapplica colore ed editabilità per sicurezza\n",
    "                viewer.layers[UNIQUE_PRED_LAYER_NAME].color = {0: 'magenta', 1: 'lime'}\n",
    "                viewer.layers[UNIQUE_PRED_LAYER_NAME].editable = False\n",
    "                print(\"Layer aggiornato.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Errore durante l'aggiornamento del layer: {e}\")\n",
    "        else:\n",
    "            print(f\"Creazione nuovo layer '{UNIQUE_PRED_LAYER_NAME}'...\")\n",
    "            try:\n",
    "                pred_layer = viewer.add_labels(\n",
    "                    final_prediction,\n",
    "                    name=UNIQUE_PRED_LAYER_NAME,\n",
    "                    opacity=0.40,\n",
    "                )\n",
    "                # Imposta colore e editabilità\n",
    "                pred_layer.color = {0: 'magenta', 1: 'lime'}\n",
    "                pred_layer.editable = False\n",
    "                print(\"Nuovo layer creato.\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Errore durante la creazione del layer: {e}\")\n",
    "\n",
    "        print(f\"\\n✓ Predizione 'unica' aggiornata sul layer '{UNIQUE_PRED_LAYER_NAME}'.\")\n",
    "    else:\n",
    "        print(\"Viewer Napari non trovato per aggiornare il layer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ecd1cf",
   "metadata": {},
   "source": [
    "## Video Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "import numpy as np\n",
    "import os\n",
    "import time # Import time just in case needed, though not strictly required by logic\n",
    "\n",
    "# Prova ad importare cv2 e tqdm, gestendo l'assenza\n",
    "try:\n",
    "    import cv2 # Per la scrittura video\n",
    "    print(\"Libreria OpenCV (cv2) trovata.\")\n",
    "except ImportError:\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"ERRORE: La libreria 'opencv-python' (cv2) non è installata.\")\n",
    "    print(\"Questa libreria è necessaria per creare il video.\")\n",
    "    print(\"Per favore, installala dal tuo terminale/console eseguendo:\")\n",
    "    print(\"  pip install opencv-python\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    # Imposta cv2 a None per interrompere l'esecuzione più avanti se manca\n",
    "    cv2 = None\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm # Per la barra di progresso (opzionale)\n",
    "    print(\"Libreria tqdm trovata (verrà mostrato il progresso).\")\n",
    "except ImportError:\n",
    "    print(\"Info: Libreria 'tqdm' non trovata. Il progresso non verrà mostrato.\")\n",
    "    # Definisci una funzione tqdm \"finta\" se non è installata\n",
    "    # così il codice non dà errore ma semplicemente itera senza barra\n",
    "    def tqdm(iterator, *args, **kwargs):\n",
    "        print(\"Inizio elaborazione frame...\")\n",
    "        return iterator\n",
    "    tqdm = tqdm # Assegna la funzione finta\n",
    "\n",
    "# --- Parametri di Configurazione ---\n",
    "\n",
    "# <<<======================================================================>>>\n",
    "# <<<    MODIFICA QUI il nome del layer da cui vuoi creare il video       >>>\n",
    "# <<<======================================================================>>>\n",
    "layer_name_to_process = \"Model Pred Unique\"  # Oppure cambia in \"Model Pred\"\n",
    "\n",
    "output_directory = \"output_videos\" # Cartella dove salvare il video\n",
    "# Crea un nome file basato sul nome del layer, sostituendo spazi\n",
    "safe_layer_name = layer_name_to_process.replace(' ', '_')\n",
    "output_filename = f\"{safe_layer_name}_BW_video.mp4\"\n",
    "output_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "fps = 10.0 # Frame al secondo del video finale (puoi aggiustare)\n",
    "\n",
    "# --- Inizio Script ---\n",
    "\n",
    "print(f\"\\nTentativo di creare un video in Bianco/Nero per il layer: '{layer_name_to_process}'\")\n",
    "print(f\"Il video verrà salvato come: '{output_path}' a {int(fps)} FPS.\")\n",
    "\n",
    "# Procedi solo se cv2 è stato importato correttamente\n",
    "if cv2 is not None:\n",
    "    video_writer = None # Inizializza a None per il blocco finally\n",
    "    try:\n",
    "        # 1. Ottieni i Dati dal Layer Napari\n",
    "        print(\"Accesso a Napari...\")\n",
    "        viewer = napari.current_viewer()\n",
    "        if viewer is None:\n",
    "            raise RuntimeError(\"Nessun viewer Napari attivo trovato.\")\n",
    "\n",
    "        if layer_name_to_process not in viewer.layers:\n",
    "            raise KeyError(f\"Il layer specificato '{layer_name_to_process}' non è presente nel viewer Napari.\")\n",
    "\n",
    "        print(f\"Accesso al layer '{layer_name_to_process}'...\")\n",
    "        prediction_layer = viewer.layers[layer_name_to_process]\n",
    "        prediction_data = prediction_layer.data # Dovrebbe essere un array (Z, H, W) con 0 e 1\n",
    "\n",
    "        # Controlli sui dati\n",
    "        if not isinstance(prediction_data, np.ndarray):\n",
    "             raise TypeError(\"I dati del layer non sono un array NumPy.\")\n",
    "        if prediction_data.ndim != 3:\n",
    "             raise ValueError(f\"I dati del layer devono avere 3 dimensioni (Z, H, W). Trovate: {prediction_data.ndim}\")\n",
    "\n",
    "        num_frames, H, W = prediction_data.shape\n",
    "        print(f\"Dati caricati: {num_frames} frames (layers), Dimensioni: {H}x{W}\")\n",
    "\n",
    "        unique_vals = np.unique(prediction_data)\n",
    "        print(f\"Valori unici trovati nei dati del layer: {unique_vals}\")\n",
    "        # Avvisa se ci sono valori inaspettati, ma prova comunque\n",
    "        if not np.all(np.isin(unique_vals, [0, 1])):\n",
    "             print(\"ATTENZIONE: I dati contengono valori diversi da 0 e 1. Verranno mappati come segue: 0->Nero, >0->Bianco.\")\n",
    "\n",
    "        # 2. Prepara il Video Writer di OpenCV\n",
    "        # Crea la cartella di output se non esiste\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "        # Definisci il codec (FOURCC) per il formato video.\n",
    "        # 'mp4v' è un buon default per .mp4, compatibile con molti sistemi.\n",
    "        # Altri comuni: 'XVID' per .avi\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "        # Crea l'oggetto VideoWriter. Nota l'ordine (W, H) per frameSize.\n",
    "        # isColor=False perché il nostro output è binario (1 canale).\n",
    "        print(f\"Inizializzazione video writer per '{output_path}'...\")\n",
    "        video_writer = cv2.VideoWriter(output_path, fourcc, fps, (W, H), isColor=False)\n",
    "\n",
    "        # Verifica se il VideoWriter è stato aperto correttamente\n",
    "        if not video_writer.isOpened():\n",
    "             # Fornisce un messaggio di errore più dettagliato se possibile\n",
    "             codec_info = \"Codec 'mp4v' potrebbe non essere supportato o mancare librerie.\" if 'mp4v' in 'mp4v' else \"Problema con il codec specificato.\"\n",
    "             raise IOError(f\"Errore critico: Impossibile aprire il file video '{output_path}' per la scrittura. {codec_info} Verifica permessi.\")\n",
    "\n",
    "        # 3. Elabora i Frame e Scrivi il Video\n",
    "        print(f\"\\nInizio elaborazione e scrittura dei {num_frames} frames...\")\n",
    "        # Itera su ogni slice (layer) lungo l'asse Z (asse 0)\n",
    "        # Usa tqdm per mostrare una barra di progresso se disponibile\n",
    "        for i in tqdm(range(num_frames), desc=f\"Creazione video\"):\n",
    "            # Estrai la slice 2D corrente\n",
    "            layer_slice = prediction_data[i] # Contiene 0 o 1\n",
    "\n",
    "            # Converti la slice in un frame uint8 Bianco e Nero\n",
    "            # Mappa: 0 -> 0 (nero), 1 -> 255 (bianco)\n",
    "            # Qualsiasi valore > 0 verrà mappato a bianco con questo metodo.\n",
    "            frame_bw = np.where(layer_slice > 0, 255, 0).astype(np.uint8)\n",
    "            # Alternativa (se sei sicuro che ci siano solo 0 e 1):\n",
    "            # frame_bw = (layer_slice * 255).astype(np.uint8)\n",
    "\n",
    "            # Scrivi il frame nel file video\n",
    "            video_writer.write(frame_bw)\n",
    "\n",
    "        # 4. Rilascia le Risorse\n",
    "        print(\"\\nFinalizzazione video...\")\n",
    "        video_writer.release() # Chiude il file video correttamente\n",
    "        print(\"Video creato con successo!\")\n",
    "        # Stampa il percorso assoluto per chiarezza\n",
    "        print(f\"File salvato in: {os.path.abspath(output_path)}\")\n",
    "\n",
    "    # Gestione degli errori specifici e generici\n",
    "    except (KeyError, ValueError, TypeError, RuntimeError, IOError) as e:\n",
    "         print(f\"\\n--- ERRORE ---\")\n",
    "         print(f\"{type(e).__name__}: {e}\")\n",
    "         print(\"Creazione del video interrotta.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- ERRORE INASPETTATO ---\")\n",
    "        print(f\"{type(e).__name__}: {e}\")\n",
    "        print(\"Creazione del video interrotta.\")\n",
    "    finally:\n",
    "        # Assicurati SEMPRE di rilasciare il writer se è stato aperto,\n",
    "        # anche se si è verificato un errore durante la scrittura dei frame.\n",
    "        if video_writer is not None and video_writer.isOpened():\n",
    "            print(\"Rilascio precauzionale del video writer...\")\n",
    "            video_writer.release()\n",
    "\n",
    "else:\n",
    "    # Messaggio se cv2 non era disponibile all'inizio\n",
    "    print(\"\\nOperazione annullata perché la libreria 'opencv-python' non è disponibile.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e2c9a",
   "metadata": {},
   "source": [
    "- Create a box with 3 buttons: initialization - training/inference - make video\n",
    "- No hard code, providing the name of the files (maybe also the saving path)\n",
    "- getting better results\n",
    "- change colors for daltonism\n",
    "- add a 3D views (see document) --> also include this into UI\n",
    "- save final results (inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce11884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "napari-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
